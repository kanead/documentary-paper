library(fmsb) # VIF
library(MASS) # for negative binomial regression
library(broom) # export regression results
library(knitr) # export regression results
library(readxl)
install.packages(modEva)
install.packages("modEva")
install.packages("modEvA")
library(modEvA)
# get pseudo R^2 values
modEvA::RsqGLM(model=m.NB.wiki)
# export regression output
tidy_m.NB.wiki <- tidy(m.NB.wiki)
tidy_m.NB.wiki
kable(tidy_m.NB.wiki)
##################################################################################
# Multiple regression wiki GLM
##################################################################################
mult.m.NB.wiki <- glm.nb(diff~seconds+Taxa+Status+Interaction+Diaries,data = wikiData)
summary(mult.m.NB.wiki)
# check for overdispersion, should be around 1, one suggested cutoff is < 1.5
summary(mult.m.NB.wiki)$deviance / summary(mult.m.NB.wiki)$df.residual
# get the coefficients back in units we can understand by undoing the link function
exp(coef(mult.m.NB.wiki))
# get pseudo R^2 values
modEvA::RsqGLM(model=mult.m.NB.wiki)
# export regression output
tidy_mult.m.NB.wiki <- tidy(mult.m.NB.wiki)
tidy_mult.m.NB.wiki
kable(tidy_mult.m.NB.wiki)
# compare the simple model with the more complex using AIC
AIC(m.NB.wiki,mult.m.NB.wiki)
##################################################################################
# Simple linear model for twitter hits of the species
##################################################################################
tweetData <- subData[complete.cases(subData$tweet), ] # drop the NAs for twitter
length(tweetData)
length(tweetData$Baseline)
m.lm.twitter<-lm(tweet~seconds,data=tweetData)
summary(m.lm.twitter)
##################################################################################
# fit negative binomial regression for tweet counts
##################################################################################
m.NB.twitter<-glm.nb(tweet~seconds,data=tweetData)
summary(m.NB.twitter)
m.lm.twitter<-lm(tweet~seconds,data=tweetData)
summary(m.lm.twitter)
# check for overdispersion, should be around 1, one suggested cutoff is < 1.5
summary(m.NB.twitter)$deviance / summary(m.NB.twitter)$df.residual
# get the coefficients back in units we can understand by undoing the link function
exp(coef(m.NB.twitter))
# get pseudo R^2 values
modEvA::RsqGLM(model=m.NB.twitter)
# export regression output
tidy_m.NB.twitter <- tidy(m.NB.twitter)
tidy_m.NB.twitter
kable(tidy_m.NB.twitter)
##################################################################################
# Multiple regression twitter GLM
##################################################################################
mult.m.NB.twitter<-glm.nb(tweet~seconds+Taxa+Status+Diaries,data=tweetData)
summary(mult.m.NB.twitter)
# check for overdispersion, should be around 1, one suggested cutoff is < 1.5
summary(mult.m.NB.twitter)$deviance / summary(mult.m.NB.twitter)$df.residual
# get the coefficients back in units we can understand by undoing the link function
exp(coef(mult.m.NB.twitter))
# get pseudo R^2 values
modEvA::RsqGLM(model=mult.m.NB.twitter)
# export regression output
tidy_mult.m.NB.twitter <- tidy(mult.m.NB.twitter)
tidy_mult.m.NB.twitter
kable(tidy_mult.m.NB.twitter)
# compare the simple model with the more complex using AIC
AIC(m.NB.twitter,mult.m.NB.twitter)
1058.052 - 1046.643
kable(tidy_m.NB.wiki)
length(wikiData$Baseline)
##################################################################################
# Multiple regression wiki GLM
##################################################################################
mult.m.NB.wiki <- glm.nb(diff~seconds+Taxa+Status+Interaction+Diaries,data = wikiData)
summary(mult.m.NB.wiki)
# get pseudo R^2 values
modEvA::RsqGLM(model=m.NB.wiki)
# compare the simple model with the more complex using AIC
AIC(m.NB.wiki,mult.m.NB.wiki)
1700.283 - 1688.911
##################################################################################
# fit poisson regression for wiki hits on with the negatives removed
##################################################################################
# drop the differences that are negative i.e. those pages that had values less
# than the median on the episode of Planet Earth 2 covered that species
length(subData$Baseline)
wikiData<-subData[subData$diff>0,]
wikiData <- droplevels(wikiData)
length(wikiData$Baseline)
length(tweetData$Baseline)
m.lm.twitter<-lm(tweet~seconds,data=tweetData)
summary(m.lm.twitter)
################################################################################### Documentary analysis
# Code for Regression Models
##################################################################################
library(ggplot2)
library(fmsb) # VIF
library(MASS) # for negative binomial regression
library(broom) # export regression results
library(knitr) # export regression results
library(readxl)
library(modEvA)
mydata<-read_excel("data/PE2_data.xlsx", sheet = "PE2 data")
subData <- mydata[,c("Baseline","Number tweets","PE2 visits","Time on screen","Taxa","Status","Interaction","Diaries")]
head(subData)
tail(subData)
# PE2 visits represents the difference between the maximum value the page had on the day
# or day after broadcast minus the baseline value from mid 2015 to mid 2016
# rename columns with spaces
names(subData)[names(subData) == 'Time on screen'] <- 'seconds'
names(subData)[names(subData) == 'Number tweets'] <- 'tweet'
names(subData)[names(subData) == 'PE2 visits'] <- 'diff'
head(subData)
# number of tweets and diff should be numeric
subData$tweet <- as.numeric(subData$tweet)
subData$diff <- as.numeric(subData$diff)
# Interaction should be a factor variable
subData$Interaction <- as.factor(subData$Interaction)
# rename prey in the Interaction column as pred to simplify
levels(subData$Interaction)[levels(subData$Interaction)=="prey"] <- "pred"
levels(subData$Interaction)
head(subData)
##################################################################################
# ANOVA of time on screen as a function of conservation Status
# Make some plots too
##################################################################################
# replace the NAs in conservation Status with 0
subData$Status[is.na(subData$Status)] <- 0
subData$Status <- as.factor(subData$Status)
levels(subData$Status)
m.aov<-aov(subData$seconds~subData$Status)
summary(m.aov)
p <- ggplot(subData, aes(x=Status, y=seconds,fill=Status)) +
geom_boxplot() +
labs(x="IUCN Status", y = "Seconds on screen")
p + scale_fill_manual(labels = c("data deficient", "least concern", "near threatened", "vulnerable","endangered","critically endangered"),values=c("#ffffff", "#009933", "#336600","#ff9900", "#cc0000", "#990000"),name="IUCN Status") + theme_classic() +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
summary(subData$Status)
round(summary(subData$Status)/sum(table(subData$Status)),3)
sum(round(summary(subData$Status)/sum(table(subData$Status)),3))
p2 <- ggplot(subData, aes(x=Status, y=tweet,fill=Status)) +
geom_boxplot() +
labs(x="IUCN Status", y = "number of tweets")
p2 + scale_fill_manual(labels = c("data deficient", "least concern", "near threatened", "vulnerable","endangered","critically endangered"),values=c("#ffffff", "#009933", "#336600","#ff9900", "#cc0000", "#990000"),name="IUCN Status") + theme_classic() +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
##################################################################################
# fit poisson regression for wiki hits on with the negatives removed
##################################################################################
# drop the differences that are negative i.e. those pages that had values less
# than the median on the episode of Planet Earth 2 covered that species
# also drop any NA values for Wikipedia
length(subData$Baseline)
wikiData<-subData[subData$diff>0,]
wikiData <- droplevels(wikiData)
wikiData <- wikiData[complete.cases(wikiData$diff), ] # drop the NAs for twitter
length(wikiData$Baseline)
# multicolinearity test
VIF(lm(wikiData$tweet~wikiData$seconds+wikiData$diff))
VIF(lm(wikiData$seconds~wikiData$tweet+wikiData$diff))
VIF(lm(wikiData$diff~wikiData$tweet+wikiData$seconds))
m.poiss.wiki <- glm(diff~seconds,data = wikiData,family=poisson)
summary(m.poiss.wiki) # overdispersed
##################################################################################
# fit negative binomial regression for wiki hits on with the negatives removed
##################################################################################
m.NB.wiki <- glm.nb(diff~seconds,data = wikiData)
summary(m.NB.wiki)
# check for overdispersion, should be around 1, one suggested cutoff is < 1.5
summary(m.NB.wiki)$deviance / summary(m.NB.wiki)$df.residual
# get the coefficients back in units we can understand by undoing the link function
exp(coef(m.NB.wiki))
# get pseudo R^2 values
modEvA::RsqGLM(model=m.NB.wiki)
# export regression output
tidy_m.NB.wiki <- tidy(m.NB.wiki)
tidy_m.NB.wiki
kable(tidy_m.NB.wiki)
##################################################################################
# Multiple regression wiki GLM
##################################################################################
mult.m.NB.wiki <- glm.nb(diff~seconds+Taxa+Status+Interaction+Diaries,data = wikiData)
summary(mult.m.NB.wiki)
plot(mult.m.NB.wiki)
# check for overdispersion, should be around 1, one suggested cutoff is < 1.5
summary(mult.m.NB.wiki)$deviance / summary(mult.m.NB.wiki)$df.residual
# get the coefficients back in units we can understand by undoing the link function
exp(coef(mult.m.NB.wiki))
# get pseudo R^2 values
modEvA::RsqGLM(model=mult.m.NB.wiki)
# export regression output
tidy_mult.m.NB.wiki <- tidy(mult.m.NB.wiki)
tidy_mult.m.NB.wiki
kable(tidy_mult.m.NB.wiki)
# compare the simple model with the more complex using AIC
AIC(m.NB.wiki,mult.m.NB.wiki)
levels(subData$Taxa)
levels(as.factor(subData$Taxa))
################################################################################### Documentary analysis
# Code for Regression Models
##################################################################################
library(ggplot2)
library(fmsb) # VIF
library(MASS) # for negative binomial regression
library(broom) # export regression results
library(knitr) # export regression results
library(readxl)
library(modEvA)
mydata<-read_excel("data/PE2_data.xlsx", sheet = "PE2 data")
subData <- mydata[,c("Baseline","Number tweets","PE2 visits","Time on screen","Taxa","Status","Interaction","Diaries")]
head(subData)
tail(subData)
# PE2 visits represents the difference between the maximum value the page had on the day
# or day after broadcast minus the baseline value from mid 2015 to mid 2016
# rename columns with spaces
names(subData)[names(subData) == 'Time on screen'] <- 'seconds'
names(subData)[names(subData) == 'Number tweets'] <- 'tweet'
names(subData)[names(subData) == 'PE2 visits'] <- 'diff'
head(subData)
# number of tweets and diff should be numeric
subData$tweet <- as.numeric(subData$tweet)
subData$diff <- as.numeric(subData$diff)
# Interaction should be a factor variable
subData$Interaction <- as.factor(subData$Interaction)
# rename prey in the Interaction column as pred to simplify
levels(subData$Interaction)[levels(subData$Interaction)=="prey"] <- "pred"
levels(subData$Interaction)
head(subData)
##################################################################################
# Documentary analysis
# Code for Regression Models
##################################################################################
library(ggplot2)
library(fmsb) # VIF
library(MASS) # for negative binomial regression
library(broom) # export regression results
library(knitr) # export regression results
library(readxl)
library(modEvA)
mydata<-read_excel("data/PE2_data.xlsx", sheet = "PE2 data")
subData <- mydata[,c("Baseline","Number tweets","PE2 visits","Time on screen","Taxa","Status","Interaction","Diaries")]
head(subData)
tail(subData)
# PE2 visits represents the difference between the maximum value the page had on the day
# or day after broadcast minus the baseline value from mid 2015 to mid 2016
# rename columns with spaces
names(subData)[names(subData) == 'Time on screen'] <- 'seconds'
names(subData)[names(subData) == 'Number tweets'] <- 'tweet'
names(subData)[names(subData) == 'PE2 visits'] <- 'diff'
head(subData)
# number of tweets and diff should be numeric
subData$tweet <- as.numeric(subData$tweet)
subData$diff <- as.numeric(subData$diff)
# Interaction should be a factor variable
subData$Interaction <- as.factor(subData$Interaction)
# rename prey in the Interaction column as pred to simplify
levels(subData$Interaction)[levels(subData$Interaction)=="prey"] <- "pred"
levels(subData$Interaction)
head(subData)
# replace the NAs in conservation Status with 0
subData$Status[is.na(subData$Status)] <- 0
subData$Status <- as.factor(subData$Status)
levels(subData$Status)
m.aov<-aov(subData$seconds~subData$Status)
summary(m.aov)
p <- ggplot(subData, aes(x=Status, y=seconds,fill=Status)) +
geom_boxplot() +
labs(x="IUCN Status", y = "Seconds on screen")
p + scale_fill_manual(labels = c("data deficient", "least concern", "near threatened", "vulnerable","endangered","critically endangered"),values=c("#ffffff", "#009933", "#336600","#ff9900", "#cc0000", "#990000"),name="IUCN Status") + theme_classic() +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
summary(subData$Status)
round(summary(subData$Status)/sum(table(subData$Status)),3)
sum(round(summary(subData$Status)/sum(table(subData$Status)),3))
p2 <- ggplot(subData, aes(x=Status, y=tweet,fill=Status)) +
geom_boxplot() +
labs(x="IUCN Status", y = "number of tweets")
p2 + scale_fill_manual(labels = c("data deficient", "least concern", "near threatened", "vulnerable","endangered","critically endangered"),values=c("#ffffff", "#009933", "#336600","#ff9900", "#cc0000", "#990000"),name="IUCN Status") + theme_classic() +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
##################################################################################
# Documentary analysis
# Code for Regression Models
##################################################################################
library(ggplot2)
library(fmsb) # VIF
library(MASS) # for negative binomial regression
library(broom) # export regression results
library(knitr) # export regression results
library(readxl)
library(modEvA)
mydata<-read_excel("data/PE2_data.xlsx", sheet = "PE2 data")
subData <- mydata[,c("Baseline","Number tweets","PE2 visits","Time on screen","Taxa","Status","Interaction","Diaries")]
head(subData)
tail(subData)
# PE2 visits represents the difference between the maximum value the page had on the day
# or day after broadcast minus the baseline value from mid 2015 to mid 2016
# rename columns with spaces
names(subData)[names(subData) == 'Time on screen'] <- 'seconds'
names(subData)[names(subData) == 'Number tweets'] <- 'tweet'
names(subData)[names(subData) == 'PE2 visits'] <- 'diff'
head(subData)
# number of tweets and diff should be numeric
subData$tweet <- as.numeric(subData$tweet)
subData$diff <- as.numeric(subData$diff)
# Interaction should be a factor variable
subData$Interaction <- as.factor(subData$Interaction)
# replace the NAs in conservation Status with 0
subData$Status[is.na(subData$Status)] <- 0
subData$Status <- as.factor(subData$Status)
# rename prey in the Interaction column as pred to simplify
levels(subData$Interaction)[levels(subData$Interaction)=="prey"] <- "pred"
levels(subData$Interaction)
head(subData)
##################################################################################
# fit poisson regression for wiki hits on with the negatives removed
##################################################################################
# drop the differences that are negative i.e. those pages that had values less
# than the median on the episode of Planet Earth 2 covered that species
# also drop any NA values for Wikipedia
length(subData$Baseline)
wikiData<-subData[subData$diff>0,]
wikiData <- droplevels(wikiData)
wikiData <- wikiData[complete.cases(wikiData$diff), ] # drop the NAs for twitter
length(wikiData$Baseline)
# multicolinearity test
VIF(lm(wikiData$tweet~wikiData$seconds+wikiData$diff))
VIF(lm(wikiData$seconds~wikiData$tweet+wikiData$diff))
VIF(lm(wikiData$diff~wikiData$tweet+wikiData$seconds))
m.poiss.wiki <- glm(diff~seconds,data = wikiData,family=poisson)
summary(m.poiss.wiki) # overdispersed
##################################################################################
# fit negative binomial regression for wiki hits on with the negatives removed
##################################################################################
m.NB.wiki <- glm.nb(diff~seconds,data = wikiData)
summary(m.NB.wiki)
##################################################################################
# Multiple regression wiki GLM
##################################################################################
mult.m.NB.wiki <- glm.nb(diff~seconds+Taxa+Status+Interaction+Diaries,data = wikiData)
summary(mult.m.NB.wiki)
# check for overdispersion, should be around 1, one suggested cutoff is < 1.5
summary(mult.m.NB.wiki)$deviance / summary(mult.m.NB.wiki)$df.residual
# get the coefficients back in units we can understand by undoing the link function
exp(coef(mult.m.NB.wiki))
# get pseudo R^2 values
modEvA::RsqGLM(model=mult.m.NB.wiki)
# export regression output
tidy_mult.m.NB.wiki <- tidy(mult.m.NB.wiki)
tidy_mult.m.NB.wiki
kable(tidy_mult.m.NB.wiki)
# compare the simple model with the more complex using AIC
AIC(m.NB.wiki,mult.m.NB.wiki)
##################################################################################
# Simple linear model for twitter hits of the species
##################################################################################
tweetData <- subData[complete.cases(subData$tweet), ] # drop the NAs for twitter
length(tweetData$Baseline)
##################################################################################
# Simple linear model for twitter hits of the species
##################################################################################
tweetData <- subData[complete.cases(subData$tweet), ] # drop the NAs for twitter
length(tweetData$Baseline)
m.lm.twitter<-lm(tweet~seconds,data=tweetData)
summary(m.lm.twitter)
plot(m.lm.twitter) # poor fit
length(tweetData$Baseline)
m.NB.twitter<-glm.nb(tweet~seconds,data=tweetData)
summary(m.NB.twitter)
plot(m.NB.twitter)
# check for overdispersion, should be around 1, one suggested cutoff is < 1.5
summary(m.NB.twitter)$deviance / summary(m.NB.twitter)$df.residual
# get the coefficients back in units we can understand by undoing the link function
exp(coef(m.NB.twitter))
# get pseudo R^2 values
modEvA::RsqGLM(model=m.NB.twitter)
# export regression output
tidy_m.NB.twitter <- tidy(m.NB.twitter)
tidy_m.NB.twitter
kable(tidy_m.NB.twitter)
##################################################################################
# Multiple regression twitter GLM
##################################################################################
mult.m.NB.twitter<-glm.nb(tweet~seconds+Taxa+Status+Diaries,data=tweetData)
summary(mult.m.NB.twitter)
plot(mult.m.NB.twitter)
# get the coefficients back in units we can understand by undoing the link function
exp(coef(mult.m.NB.twitter))
# get pseudo R^2 values
modEvA::RsqGLM(model=mult.m.NB.twitter)
# export regression output
tidy_mult.m.NB.twitter <- tidy(mult.m.NB.twitter)
tidy_mult.m.NB.twitter
kable(tidy_mult.m.NB.twitter)
# compare the simple model with the more complex using AIC
AIC(m.NB.twitter,mult.m.NB.twitter)
1067.727 - 1056.030
kable(tidy_m.NB.twitter)
##################################################################################
# Documentary analysis
# Code for Regression Models
##################################################################################
library(ggplot2)
library(fmsb) # VIF
library(MASS) # for negative binomial regression
library(broom) # export regression results
library(knitr) # export regression results
library(readxl)
library(modEvA)
mydata<-read_excel("data/PE2_data.xlsx", sheet = "PE2 data")
subData <- mydata[,c("Baseline","Number tweets","PE2 visits","Time on screen","Taxa","Status","Interaction","Diaries")]
head(subData)
tail(subData)
# PE2 visits represents the difference between the maximum value the page had on the day
# or day after broadcast minus the baseline value from mid 2015 to mid 2016
# rename columns with spaces
names(subData)[names(subData) == 'Time on screen'] <- 'seconds'
names(subData)[names(subData) == 'Number tweets'] <- 'tweet'
names(subData)[names(subData) == 'PE2 visits'] <- 'diff'
head(subData)
# number of tweets and diff should be numeric
subData$tweet <- as.numeric(subData$tweet)
subData$diff <- as.numeric(subData$diff)
# Interaction should be a factor variable
subData$Interaction <- as.factor(subData$Interaction)
# replace the NAs in conservation Status with 0
subData$Status[is.na(subData$Status)] <- 0
subData$Status <- as.factor(subData$Status)
# rename prey in the Interaction column as pred to simplify
levels(subData$Interaction)[levels(subData$Interaction)=="prey"] <- "pred"
levels(subData$Interaction)
head(subData)
##################################################################################
# fit poisson regression for wiki hits on with the negatives removed
##################################################################################
# drop the differences that are negative i.e. those pages that had values less
# than the median on the episode of Planet Earth 2 covered that species
# also drop any NA values for Wikipedia
length(subData$Baseline)
wikiData<-subData[subData$diff>0,]
wikiData <- droplevels(wikiData)
wikiData <- wikiData[complete.cases(wikiData$diff), ] # drop the NAs for twitter
length(wikiData$Baseline)
##################################################################################
# fit negative binomial regression for tweet counts
##################################################################################
tweetData <- subData[complete.cases(subData$tweet), ] # drop the NAs for twitter
length(tweetData$Baseline)
kable(tidy_mult.m.NB.twitter)
levels(tweetData$Status)
kable(tidy_mult.m.NB.wiki)
kable(tidy_mult.m.NB.wiki)
kable(tidy_m.NB.wiki)
kable(tidy_mult.m.NB.wiki)
kable(tidy_m.NB.wiki)
kable(tidy_mult.m.NB.wiki)
kable(tidy_m.NB.twitter)
kable(tidy_mult.m.NB.twitter)
# episode 6 - 11 December 2016	2016-12-11
#                               2016-12-12
###################################################################################
library(data.table)
library(plyr)
library(AnomalyDetection)
# read in the data for the charity donations
bornFree<-read.csv("data/bornfree_donations.csv",header = T,sep = ",")
arkive<-read.csv("data/arkive_donations.csv",header = T,sep = ",")
# change to date format
bornFree$date<-as.POSIXct(strptime(bornFree$date,"%d/%m/%Y"))
arkive$date<-as.POSIXct(strptime(arkive$date,"%d/%m/%Y"))
# test for anomalies
# BornFree first
resBornFree = AnomalyDetectionTs(data.frame(bornFree[1:2]), max_anoms=0.01, direction='both', plot=TRUE)
resBornFree$plot
resBornFree$anoms
resBornFree$anoms$timestamp
# now Arkive
resarkive = AnomalyDetectionTs(data.frame(arkive[1:2]), max_anoms=0.01, direction='both', plot=TRUE)
resarkive$plot
resarkive$anoms
resarkive$anoms$timestamp
plot(arkive$date,arkive$standardised_amount)
plot(arkive$date,arkive$standardised_amount,xlab = "Date", ylab = "Standardised donations")
library(ggplot2)
ggplot(aes(arkive$date,arkive$standardised_amount)) + geom_point()
ggplot(arkive,aes(date,standardised_amount)) + geom_point()
ggplot(arkive,aes(date,standardised_amount)) + geom_point() + xlab("Date")
ggplot(arkive,aes(date,standardised_amount)) + geom_point() + xlab("Date") + ylab("Standardised donations")
ggplot(arkive,aes(date,standardised_amount)) + geom_point() + xlab("Date") + ylab("Standardised donations") + theme_classic()
ggplot(bornFree,aes(date,standardised_amount)) + geom_point() + xlab("Date") + ylab("Standardised donations") + theme_classic()
mydata <- rbind(bornFree,arkive)
head(mydata)
mydata <- merge(arkive,bornFree)
head(mydata)
mydata
mydata <- merge(arkive,bornFree,all = T)
head(mydata)
mydata <- merge(arkive,bornFree,all.x = T)
head(mydata)
ggplot(arkive,aes(date,standardised_amount)) + geom_point() + xlab("Date") + ylab("Mean standardised donations") + theme_classic()
