impact22$summary$AbsEffect[1],
impact23$summary$AbsEffect[1],
impact24$summary$AbsEffect[1],
impact25$summary$AbsEffect[1],
impact26$summary$AbsEffect[1],
impact27$summary$AbsEffect[1],
impact28$summary$AbsEffect[1],
impact29$summary$AbsEffect[1],
impact30$summary$AbsEffect[1],
impact31$summary$AbsEffect[1],
impact32$summary$AbsEffect[1],
impact33$summary$AbsEffect[1],
impact34$summary$AbsEffect[1],
impact35$summary$AbsEffect[1],
impact36$summary$AbsEffect[1],
impact37$summary$AbsEffect[1],
impact38$summary$AbsEffect[1],
impact39$summary$AbsEffect[1],
impact40$summary$AbsEffect[1],
impact41$summary$AbsEffect[1],
impact42$summary$AbsEffect[1],
impact43$summary$AbsEffect[1]
)
# upper values for average response
upper <- c(
impact1$summary$AbsEffect.upper[1],
impact2$summary$AbsEffect.upper[1],
impact3$summary$AbsEffect.upper[1],
impact4$summary$AbsEffect.upper[1],
impact5$summary$AbsEffect.upper[1],
impact6$summary$AbsEffect.upper[1],
impact7$summary$AbsEffect.upper[1],
impact8$summary$AbsEffect.upper[1],
impact9$summary$AbsEffect.upper[1],
impact10$summary$AbsEffect.upper[1],
impact11$summary$AbsEffect.upper[1],
impact12$summary$AbsEffect.upper[1],
impact13$summary$AbsEffect.upper[1],
impact14$summary$AbsEffect.upper[1],
impact15$summary$AbsEffect.upper[1],
impact16$summary$AbsEffect.upper[1],
impact17$summary$AbsEffect.upper[1],
impact18$summary$AbsEffect.upper[1],
impact19$summary$AbsEffect.upper[1],
impact20$summary$AbsEffect.upper[1],
impact21$summary$AbsEffect.upper[1],
impact22$summary$AbsEffect.upper[1],
impact23$summary$AbsEffect.upper[1],
impact24$summary$AbsEffect.upper[1],
impact25$summary$AbsEffect.upper[1],
impact26$summary$AbsEffect.upper[1],
impact27$summary$AbsEffect.upper[1],
impact28$summary$AbsEffect.upper[1],
impact29$summary$AbsEffect.upper[1],
impact30$summary$AbsEffect.upper[1],
impact31$summary$AbsEffect.upper[1],
impact32$summary$AbsEffect.upper[1],
impact33$summary$AbsEffect.upper[1],
impact34$summary$AbsEffect.upper[1],
impact35$summary$AbsEffect.upper[1],
impact36$summary$AbsEffect.upper[1],
impact37$summary$AbsEffect.upper[1],
impact38$summary$AbsEffect.upper[1],
impact39$summary$AbsEffect.upper[1],
impact40$summary$AbsEffect.upper[1],
impact41$summary$AbsEffect.upper[1],
impact42$summary$AbsEffect.upper[1],
impact43$summary$AbsEffect.upper[1]
)
summaryData <- data.frame(cbind(lower,averageEffect,upper,summary_causal_impact[1:43,]
))
head(summaryData)
# are the significant effects positive?
summaryData$positive <- ifelse(summaryData$averageEffect>0,1,0)
# 15 23 35 are the significant negatives
plot(impact2)
plot(impact14)
plot(impact15)
plot(impact23)
plot(impact28)
plot(impact35)
plot(impact38)
summaryData
sum(summaryData$positive>0)
sum(summaryData$positive>0 & summaryData$significance>0)
# export
write.csv(summaryData,"data/caual_impact_summary.csv",row.names = F)
# how many significant and positive results?
sum(summaryData$positive>0 & summaryData$significance>0)
plot(impact25)
################################################################################### Documentary analysis
# Code for Regression Models
##################################################################################
library(ggplot2)
library(fmsb) # VIF
library(MASS) # for negative binomial regression
library(broom) # export regression results
library(knitr) # export regression results
mydata<-read.csv("data/master.csv", header = T,sep=",")
head(mydata)
tail(mydata)
subData <- mydata[,c("baseline_2015","tweet","diff","seconds","taxa_gen","status","pred_prey","diaries")]
head(subData)
tail(subData)
# diff represents the difference between the maximum value the page had on the day
# or day after broadcast minus the baseline value from mid 2015 to mid 2016
# rename prey in the pred_prey as pred
levels(subData$pred_prey)[levels(subData$pred_prey)=="prey"] <- "pred"
head(subData)
# recode actinopterygii, amphibia, arachnida, diplopoda, malacostraca
levels(subData$taxa_gen)[levels(subData$taxa_gen)=="actinopterygii"] <- "reptilia"
levels(subData$taxa_gen)[levels(subData$taxa_gen)=="amphibia"] <- "reptilia"
levels(subData$taxa_gen)[levels(subData$taxa_gen)=="arachnida"] <- "insecta"
levels(subData$taxa_gen)[levels(subData$taxa_gen)=="diplopoda"] <- "insecta"
levels(subData$taxa_gen)[levels(subData$taxa_gen)=="malacostraca"] <- "insecta"
head(subData)
levels(subData$taxa_gen)
##################################################################################
# ANOVA of time on screen as a function of conservation status
##################################################################################
# replace the NAs in conservation status with 0
subData$status[is.na(subData$status)] <- 0
subData$status <- as.factor(subData$status)
levels(subData$status)
m.aov<-aov(subData$seconds~subData$status)
summary(m.aov)
p <- ggplot(subData, aes(x=status, y=seconds,fill=status)) +
geom_boxplot() +
labs(x="IUCN status", y = "Seconds on screen")
p + scale_fill_manual(labels = c("data deficient", "least concern", "near threatened", "vulnerable","endangered","critically endangered"),values=c("#ffffff", "#009933", "#336600","#ff9900", "#cc0000", "#990000"),name="IUCN status") + theme_classic() +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
summary(subData$status)
round(summary(subData$status)/sum(table(subData$status)),3)
sum(round(summary(subData$status)/sum(table(subData$status)),3))
p2 <- ggplot(subData, aes(x=status, y=tweet,fill=status)) +
geom_boxplot() +
labs(x="IUCN status", y = "number of tweets")
p2 + scale_fill_manual(labels = c("data deficient", "least concern", "near threatened", "vulnerable","endangered","critically endangered"),values=c("#ffffff", "#009933", "#336600","#ff9900", "#cc0000", "#990000"),name="IUCN status") + theme_classic() +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
##################################################################################
# Simple linear model for wikipedia hits
##################################################################################
# find out which explanatory variables have NAs
apply(subData, 2, function(x){any(is.na(x))})
m.lm.wiki <- lm(diff~seconds,data = subData)
summary(m.lm.wiki)
plot(m.lm.wiki) # poor fit
##################################################################################
# fit poisson regression for wiki hits on with the negatives removed
##################################################################################
# drop the differences that are negative i.e. those pages that had values less
# than the median on the episode of Planet Earth 2 covered that species
length(subData$baseline_2015)
wikiData<-subData[subData$diff>0,]
wikiData <- droplevels(wikiData)
length(wikiData$baseline_2015)
# multicolinearity test
VIF(lm(wikiData$tweet~wikiData$seconds+wikiData$diff))
VIF(lm(wikiData$seconds~wikiData$tweet+wikiData$diff))
VIF(lm(wikiData$diff~wikiData$tweet+wikiData$seconds))
m.poiss.wiki <- glm(diff~seconds,data = wikiData,family=poisson)
summary(m.poiss.wiki) # overdispersed
##################################################################################
# fit negative binomial regression for wiki hits on with the negatives removed
##################################################################################
m.NB.wiki <- glm.nb(diff~seconds,data = wikiData)
summary(m.NB.wiki)
plot(m.NB.wiki)
# get the coefficients back in units we can understand by undoing the link function
exp(coef(m.NB.wiki))
# get pseudo R^2 values
modEvA::RsqGLM(model=m.NB.wiki)
# export regression output
tidy_m.NB.wiki <- tidy(m.NB.wiki)
tidy_m.NB.wiki
kable(tidy_m.NB.wiki)
##################################################################################
# Multiple regression wiki GLM
##################################################################################
mult.m.NB.wiki <- glm.nb(diff~seconds+taxa_gen+status+pred_prey+diaries,data = wikiData)
summary(mult.m.NB.wiki)
plot(mult.m.NB.wiki)
# get the coefficients back in units we can understand by undoing the link function
exp(coef(mult.m.NB.wiki))
# get pseudo R^2 values
modEvA::RsqGLM(model=mult.m.NB.wiki)
# export regression output
tidy_mult.m.NB.wiki <- tidy(mult.m.NB.wiki)
tidy_mult.m.NB.wiki
kable(tidy_mult.m.NB.wiki)
# compare the simple model with the more complex using AIC
AIC(m.NB.wiki,mult.m.NB.wiki)
##################################################################################
# Simple linear model for twitter hits of the species
##################################################################################
tweetData <- subData[complete.cases(subData), ] # drop the NAs for twitter
m.lm.twitter<-lm(tweet~seconds,data=tweetData)
summary(m.lm.twitter)
##################################################################################
# fit negative binomial regression for tweet counts
##################################################################################
m.NB.twitter<-glm.nb(tweet~seconds,data=tweetData)
summary(m.NB.twitter)
length(tweetData$baseline_2015)
################################################################################## Documentary analysis
# Code to calculate causal impact of Planet Earth 2 on the long terms trends of
# the Wikipedia articles of species featured on the show that also had anomalies
#################################################################################
# http://google.github.io/CausalImpact/CausalImpact.html
# libraries
library(dplyr)
library(CausalImpact)
library(pageviews)
library(data.table)
library(textclean)
library(tidyr)
# load in data
# Use the data for Planet Earth 1 control time series
mydata<-read.csv("data/mentioned_names_both.csv",header = T,sep = ",")
levels(mydata$name)
head(mydata)
# drop the duplicates
data = mydata %>% distinct(name)
article <- data$name
article <- droplevels(article)
# collect the time series of Wikiepia hits
wikiFunc <- function(x){article_pageviews(project = "en.wikipedia", article = x
, start = as.Date('2016-05-01'),
end = as.Date("2017-10-31")
, user_type = "user", platform = c("desktop", "mobile-web"))
}
# collect the wiki hits
wikiHits <-article %>% wikiFunc
# can group the mobile and desktop data
dataTableOutput<-setDT(wikiHits)[, .(sumy=sum(views)), by = .(article,date)]
# 66 + 42 for sloth 2016-05-01
newdata<-data.frame(dataTableOutput)
names(newdata)[names(newdata) == 'sumy'] <- 'views'
#newdata$date<-as.POSIXct(strptime(newdata$date,"%Y-%m-%d"))
newdata<-droplevels(newdata)
head(newdata)
# rename the combined dataframe
wikiHits <- newdata
# clean up the data
# keep the columns we want
wikiHits <- wikiHits[,c("article","date","views")]
wikiHits$article <- as.factor(wikiHits$article)
# match up the species names to the series they featured in
colnames(mydata)[which(names(mydata) == "name")] <- "article"
wikiHits <- left_join(wikiHits, mydata, by=("article"))
head(wikiHits)
tail(wikiHits)
# remove confusing character strings
wikiHits$article<-strip(wikiHits$article, char.keep = "~~", digit.remove = TRUE,
apostrophe.remove = TRUE, lower.case = TRUE)
wikiHits$article <- as.factor(wikiHits$article)
levels(wikiHits$article)
head(wikiHits)
# drop species that don't have the proper time frame of 549 entries
tapply(wikiHits$views,wikiHits$article,length)
wikiHits<-wikiHits[! wikiHits$article %in% c("crab", "southernafricanvleirat","seychellesfody","jacksonswidowbird","sunflowerseastar","indri","yellowgoatfish","elephant","lion"), ]
wikiHits$article <- droplevels(wikiHits$article)
head(wikiHits)
levels(wikiHits$article)
# split the data up into a Planet Earth 2 dataset and a Planet Earth 1 dataset
splitData <- split(wikiHits, wikiHits$series)
wikiHitsPE1 <- splitData$PE1
wikiHitsPE1<-droplevels(wikiHitsPE1)
wikiHitsPE2 <- splitData$PE2
wikiHitsPE2<-droplevels(wikiHitsPE2)
# From long format to wide format
# The arguments to spread():
# - data: Data object
# - key: Name of column containing the new column names
# - value: Name of column containing values
data_widePE1 <- spread(wikiHitsPE1, article, views)
head(data_widePE1)
data_widePE2 <- spread(wikiHitsPE2, article, views)
head(data_widePE2)
# plot the basic time series
plot(data_widePE2$komododragon~data_widePE2$date,pch=16, type="l")
plot(data_widePE1$bactriancamel~data_widePE1$date,pch=16, type="l")
# specify the comparative periods
pre.period <- as.Date(c("2016-05-01", "2016-06-30"))
post.period <- as.Date(c("2017-05-01", "2017-06-30"))
# move date to the end of the dataframe to make the loop easier to work with
data_widePE1<-data_widePE1[,c(colnames(data_widePE1)[colnames(data_widePE1)!='date'],'date')]
data_widePE1$series <- NULL
head(data_widePE1)
data_widePE2<-data_widePE2[,c(colnames(data_widePE2)[colnames(data_widePE2)!='date'],'date')]
data_widePE2$series <- NULL
head(data_widePE2)
time.points <- data_widePE1$date
class(time.points)
time.points <- as.Date(time.points)
head(time.points)
# function to combine the control time series
combineMedianComp <- function(data1, data2, col, n){
if(nrow(data1) > nrow(data2)) stop("Rows in 'data2' need to be greater or equal to rows in 'data1'")
medRef <- median(data1[[col]], na.rm = T, ) # median of desired column
medComp <- sapply(data2, function(x){abs(medRef - median(x, na.rm = T))}) # vector with medians for each columns in data2 ('wiki_2')
cols <- names(sort(medComp)[seq_len(n)]) # sort this vector in ascending order, select top n
d2 <- data2[, c(cols)] # select columns in data2 that have medians closest to 'medRef'
d2 <- d2[sample(seq_len(nrow(d2)), size = nrow(data1), replace = F), ] # subset column as to match those in data1
# merge data
res <- do.call(cbind, list(data1[col], d2))
return(res)
}
# run through all of the Planet Earth 2 species' Wikipedia articles and
# compare them to a control based on the Planet Earth 1 data that is
# closest in median value
##################################################################################
# Documentary analysis
# Code extracts anomalies for the wikipedia articles of species featured in Planet # Earth 2
##################################################################################
# https://github.com/twitter/AnomalyDetection/blob/master/R/detect_anoms.R
# https://www.r-bloggers.com/anomaly-detection-in-r/
# involves taking the max absolute difference from the detrended sample mean in
# terms of standard deviations, remove it and repeat until you have your
# collection of x outliers.
# deleted some species because of data length: cheetah, darkling beetles, red crab
# Air dates
# episode 1 - 6 November 2016   2016-11-06
#                               2016-11-07
# episode 2 - 13 November 2016	2016-11-13
#                               2016-11-14
# episode 3 - 20 November 2016	2016-11-20
#                               2016-11-21
# episode 4 - 27 November 2016	2016-11-27
#                               2016-11-28
# episode 5 - 4 December 2016 	2016-12-04
#                               2016-12-05
# episode 6 - 11 December 2016	2016-12-11
#                               2016-12-12
##################################################################################
library(data.table)
library(plyr)
library(AnomalyDetection)
# library(dplyr) # plyr and dplyr don't play well together
# detach("package:dplyr", unload=TRUE)
# read in the data for the species names
output<-read.csv("data/output.csv",header = T,sep = ",")
# set the date class, sometimes we need different formats
# "%d-%m-%Y" "%d/%m/%Y" "%Y-%m-%d"
head(output)
output$date<-as.POSIXct(strptime(output$date,"%Y-%m-%d"))
head(output)
# select mobile or desktop or combined access
# we use the combined data
# Mobile
# mobileOutput <- output[output$access=="mobile-web" , ]
# mobileOutput<-droplevels(mobileOutput)
# head(mobileOutput)
# newdata <- mobileOutput[c(3,7:8)]
# head(newdata)
# Desktop
# deskOutput <- output[output$access=="desktop" , ]
# deskOutput<-droplevels(deskOutput)
# head(deskOutput)
# newdata <- deskOutput[c(3,7:8)]
# Combined
# can group the mobile and desktop data
dataTableOutput<-setDT(output)[, .(sumy=sum(views)), by = .(article,date)]
newdata<-data.frame(dataTableOutput)
names(newdata)[names(newdata) == 'sumy'] <- 'views'
newdata<-droplevels(newdata)
head(newdata)
##################################################################################
# collect the species mentioned in episode 1
episode1 <- c("Pygmy_three-toed_sloth","Komodo_dragon","Lemur","Indri","Ring-tailed_lemur","Bamboo_lemur",
"Sifaka","Marine_iguana","Crab","Lizard", "Colubridae" ,"Snares_penguin","Shearwater","Buller's_albatross",
"Fairy_tern","Seychelles_fody","Noddy_(tern)","Yellow_crazy_ant","Chinstrap_penguin",
"Skua", "Christmas_Island_red_crab")
# keep only the species featured in this episode
newdata1<-newdata[newdata$article %in% episode1,]
newdata1<-droplevels(newdata1)
# episode 1 function
airedAnomDataDays1<-ddply(newdata1, "article", function(x) {
res = AnomalyDetectionTs((data.frame(x[2:3])), max_anoms=0.01, direction='both', plot=TRUE)
# determine if the anomalies are between 2 dates
anomalies<-ifelse(strptime((res$anoms$timestamp), format = "%Y-%m-%d")
== strptime(as.Date("2016-11-06"), format = "%Y-%m-%d") |
strptime((res$anoms$timestamp), format = "%Y-%m-%d")
== strptime(as.Date("2016-11-07"), format = "%Y-%m-%d"),1,0)
# find the sum total of the ones that are
sum(anomalies)
})
##################################################################################
# collect the species mentioned in episode 2
episode2 <- c("Snow_Leopard","Nubian_ibex","Red_fox","Golden_eagle","Corvidae",
"Grizzly_bear","Marmot","Bobcat","Mouse","Coyote", "Goldeneye_(duck)","Squirrel","Lagidium",
"Flamingo")
# keep only the species featured in this episode
newdata2<-newdata[newdata$article %in% episode2,]
newdata2<-droplevels(newdata2)
# episode 2 function
airedAnomDataDays2<-ddply(newdata2, "article", function(x) {
res = AnomalyDetectionTs((data.frame(x[2:3])), max_anoms=0.01, direction='both', plot=TRUE)
# determine if the anomalies are between 2 dates
anomalies<-ifelse(strptime((res$anoms$timestamp), format = "%Y-%m-%d")
== strptime(as.Date("2016-11-13"), format = "%Y-%m-%d") |
strptime((res$anoms$timestamp), format = "%Y-%m-%d")
== strptime(as.Date("2016-11-14"), format = "%Y-%m-%d"),1,0)
# find the sum total of the ones that are
sum(anomalies)
})
##################################################################################
# collect the species mentioned in episode 3
episode3 <- c("Indri","Spider_monkey","Draco_(genus)","Hummingbird", "Sword-billed_hummingbird","River_dolphin",
"Capybara","Giant_otter","Caiman","Jaguar","Uroplatus","Click_beetle","Railroad_worm","Glass_frog",
"Millipede","Red_bird-of-paradise","Wilson's_bird-of-paradise", "Rat", "Spider", "Fire_ant", "Wasp")
# keep only the species featured in this episode
newdata3<-newdata[newdata$article %in% episode3,]
newdata3<-droplevels(newdata3)
# episode 3 function
airedAnomDataDays3<-ddply(newdata3, "article", function(x) {
res = AnomalyDetectionTs((data.frame(x[2:3])), max_anoms=0.01, direction='both', plot=TRUE)
# determine if the anomalies are between 2 dates
anomalies<-ifelse(strptime((res$anoms$timestamp), format = "%Y-%m-%d")
== strptime(as.Date("2016-11-20"), format = "%Y-%m-%d") |
strptime((res$anoms$timestamp), format = "%Y-%m-%d")
== strptime(as.Date("2016-11-21"), format = "%Y-%m-%d"),1,0)
# find the sum total of the ones that are
sum(anomalies)
})
##################################################################################
# collect the species mentioned in episode 4
episode4 <- c("Lion","Oryx","Giraffe","Harris's_hawk","Ground_squirrel","Butcherbird","Locust","Zebra",
"Elephant","Sandgrouse","Pale_chanting_goshawk","Golden_mole","Desert_long-eared_bat",
"Deathstalker", "Darkling_beetle", "Pachydactylus_rangei" ,"Namaqua_chameleon")
# keep only the species featured in this episode
newdata4<-newdata[newdata$article %in% episode4,]
newdata4<-droplevels(newdata4)
# episode 4 function
airedAnomDataDays4<-ddply(newdata4, "article", function(x) {
res = AnomalyDetectionTs((data.frame(x[2:3])), max_anoms=0.01, direction='both', plot=TRUE)
# determine if the anomalies are between 2 dates
anomalies<-ifelse(strptime((res$anoms$timestamp), format = "%Y-%m-%d")
== strptime(as.Date("2016-11-27"), format = "%Y-%m-%d") |
strptime((res$anoms$timestamp), format = "%Y-%m-%d")
== strptime(as.Date("2016-11-28"), format = "%Y-%m-%d"),1,0)
# find the sum total of the ones that are
sum(anomalies)
})
##################################################################################
# collect the species mentioned in episode 5
episode5 <- c("Saiga_antelope","Lion","African_buffalo","Micromys","Barn_owl","Southern_carmine_bee-eater",
"Kori_bustard","Ostrich","Elephant","Serval","Southern_African_vlei_rat",
"Wildebeest","Jackson's_widowbird","Atta_(genus)", "Amitermes_meridionalis", "Termite","Giant_anteater","Bison","Fox",
"Vole", "Caribou","Arctic_wolf","Rhinoceros","Wild_water_buffalo","Sloth_bear", "Tiger")
# keep only the species featured in this episode
newdata5<-newdata[newdata$article %in% episode5,]
newdata5<-droplevels(newdata5)
# episode 5 function
airedAnomDataDays5<-ddply(newdata5, "article", function(x) {
res = AnomalyDetectionTs((data.frame(x[2:3])), max_anoms=0.01, direction='both', plot=TRUE)
# determine if the anomalies are between 2 dates
anomalies<-ifelse(strptime((res$anoms$timestamp), format = "%Y-%m-%d")
== strptime(as.Date("2016-12-04"), format = "%Y-%m-%d") |
strptime((res$anoms$timestamp), format = "%Y-%m-%d")
== strptime(as.Date("2016-12-05"), format = "%Y-%m-%d"),1,0)
# find the sum total of the ones that are
sum(anomalies)
})
##################################################################################
# collect the species mentioned in episode 6
episode6 <- c("Colobinae","Peregrine_falcon","Leopard","Starling","Great_bowerbird","Raccoon",
"Rhesus_macaque","Spotted_hyena","Pigeon","Wels_catfish","Hawksbill_sea_turtle",
"Crab", "Smooth-coated_otter")
# keep only the species featured in this episode
newdata6<-newdata[newdata$article %in% episode6,]
newdata6<-droplevels(newdata6)
# episode 6 function
airedAnomDataDays6<-ddply(newdata6, "article", function(x) {
res = AnomalyDetectionTs((data.frame(x[2:3])), max_anoms=0.01, direction='both', plot=TRUE)
# determine if the anomalies are between 2 dates
anomalies<-ifelse(strptime((res$anoms$timestamp), format = "%Y-%m-%d")
== strptime(as.Date("2016-12-11"), format = "%Y-%m-%d") |
strptime((res$anoms$timestamp), format = "%Y-%m-%d")
== strptime(as.Date("2016-12-12"), format = "%Y-%m-%d"),1,0)
# find the sum total of the ones that are
sum(anomalies)
})
##################################################################################
# combine the data for each of the episodes
airedAnomDataDays1
airedAnomDataDays2
airedAnomDataDays3
airedAnomDataDays4
airedAnomDataDays5
airedAnomDataDays6
# identify names that appear in more than one episode
# rename them so we can distinguish
airedAnomDataDays3$article <- gsub("Indri", "Indri_3", airedAnomDataDays3$article)
airedAnomDataDays5$article <- gsub("Elephant", "Elephant_5", airedAnomDataDays5$article)
airedAnomDataDays5$article <- gsub("Lion", "Lion_5", airedAnomDataDays5$article)
airedAnomDataDays6$article <- gsub("Crab", "Crab_6", airedAnomDataDays6$article)
allDataCombo<-rbind.fill(airedAnomDataDays1,airedAnomDataDays2,airedAnomDataDays3,airedAnomDataDays4,airedAnomDataDays5,airedAnomDataDays6)
# order alphabetically
allDataCombo <- allDataCombo[order(allDataCombo$article),]
# how many of them had anomalies on the airdates?
sum(allDataCombo$V1>0)
sum(allDataCombo$V1>0) / length(allDataCombo$V1) * 100
# export the data
write.csv(allDataCombo,file="data/anomalies_PE2_combined.csv", row.names=FALSE)
length(allDataCombo$V1)
# export the data
write.csv(allDataCombo,file="data/anomalies_PE2_combined.csv", row.names=FALSE)
